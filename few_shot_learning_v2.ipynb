{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sized\n",
    "\n",
    "\n",
    "class FewShotDataset:\n",
    "    def __init__(self, root, phase, n_shot, n_eval, transform=None):\n",
    "\n",
    "        self.root = os.path.join(root, phase)\n",
    "        self.labels = sorted(os.listdir(self.root))\n",
    "        self.n_shot = n_shot\n",
    "        self.n_eval = n_eval\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get data for a single class\n",
    "        class_dir = os.path.join(self.root, self.labels[idx])\n",
    "        class_data = [(Image.open(os.path.join(class_dir, fname)), idx) for fname in os.listdir(class_dir)]\n",
    "\n",
    "        # Separate training and evaluation data\n",
    "        n_total = len(class_data)\n",
    "        n_train = self.n_shot\n",
    "        n_eval = self.n_eval\n",
    "        train_data = class_data[:n_train]\n",
    "        eval_data = class_data[n_train:n_train+n_eval]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform is not None:\n",
    "            train_data = [(self.transform(x), y) for x, y in train_data]\n",
    "            eval_data = [(self.transform(x), y) for x, y in eval_data]\n",
    "\n",
    "        # Combine training and evaluation data\n",
    "        episode_data = train_data + eval_data\n",
    "\n",
    "        # Create episode batch\n",
    "        episode_x = torch.stack([x for x, y in episode_data], dim=0)\n",
    "        episode_y = torch.LongTensor([y for x, y in episode_data])\n",
    "\n",
    "        return episode_x, episode_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "class ClassSet(data.Dataset):\n",
    "\n",
    "    def __init__(self, images, label, transform=None):\n",
    "        self.images = images\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.images[idx], 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        return image, self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def process_data(data):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225])\n",
    "    transform1 = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(dat['image_size']),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    train_set = Episode(dat['data_root'], 'train', dat['n_shot'], dat['n_eval'],transform = transform1)\n",
    "    validation_set = Episode(dat['data_root'],'val',dat['n_shot'], dat['n_eval'], transform=transform1)\n",
    "    test_set = Episode(dat['data_root'],'test',dat['n_shot'], dat['n_eval'], transform=transform1)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_set, num_workers=dat['n_workers'])\n",
    "    validation_loader = data.DataLoader(validation_set, num_workers=2)\n",
    "    test_loader= data.DataLoader(test_set, num_workers=2)\n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(dat):\n",
    "    mode = dat['mode']\n",
    "    save_root = dat['save']\n",
    "    log_freq = dat['log_freq']\n",
    "\n",
    "    if mode == 'train':\n",
    "        if not os.path.exists(save_root):\n",
    "            os.mkdir(save_root)\n",
    "        filename = os.path.join(save_root, 'console.log')\n",
    "        logging.basicConfig(level=logging.DEBUG,\n",
    "            format='%(asctime)s.%(msecs)03d - %(message)s',\n",
    "            datefmt='%b-%d %H:%M:%S',\n",
    "            filename=filename,\n",
    "            filemode='w')\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        console.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logging.getLogger('').addHandler(console)\n",
    "\n",
    "        logging.info(\"Logger created at {}\".format(filename))\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO,\n",
    "            format='%(asctime)s.%(msecs)03d - %(message)s',\n",
    "            datefmt='%b-%d %H:%M:%S')\n",
    "\n",
    "    logging.info(\"Random Seed: {}\".format(dat['seed']))\n",
    "\n",
    "    stats = {'train': {'loss': [], 'acc': []}} if mode == 'train' else {'eval': {'loss': [], 'acc': []}}\n",
    "\n",
    "    return stats\n",
    "\n",
    "def reset_stats(stats):\n",
    "    if 'train' in stats:\n",
    "        stats['train']['loss'] = []\n",
    "        stats['train']['acc'] = []\n",
    "    if 'eval' in stats:\n",
    "        stats['eval']['loss'] = []\n",
    "        stats['eval']['acc'] = []\n",
    "\n",
    "def log_batch_info(stats, kwargs, log_freq):\n",
    "    if kwargs['phase'] == 'train':\n",
    "        stats['train']['loss'].append(kwargs['loss'])\n",
    "        stats['train']['acc'].append(kwargs['acc'])\n",
    "\n",
    "        if kwargs['eps'] % log_freq == 0 and kwargs['eps'] != 0:\n",
    "            loss_mean = np.mean(stats['train']['loss'])\n",
    "            acc_mean = np.mean(stats['train']['acc'])\n",
    "            log_info(\"[{:5d}/{:5d}] loss: {:6.4f} ({:6.4f}), acc: {:6.3f}% ({:6.3f}%)\".format(\\\n",
    "                kwargs['eps'], dat['episode_val'], kwargs['loss'], loss_mean, kwargs['acc'], acc_mean))\n",
    "\n",
    "    elif kwargs['phase'] == 'eval':\n",
    "        stats['eval']['loss'].append(kwargs['loss'])\n",
    "        stats['eval']['acc'].append(kwargs['acc'])\n",
    "\n",
    "    elif kwargs['phase'] == 'evaldone':\n",
    "        loss_mean = np.mean(stats['eval']['loss'])\n",
    "        loss_std = np.std(stats['eval']['loss'])\n",
    "        acc_mean = np.mean(stats['eval']['acc'])\n",
    "        acc_std = np.std(stats['eval']['acc'])\n",
    "        log_info(\"[{:5d}] Eval ({:3d} episode) - loss: {:6.4f} +- {:6.4f}, acc: {:6.3f} +- {:5.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_meta(eps, data_loader, model_w_grad, model_wo_grad, metalearner, config, metrics):\n",
    "    for i, (x, y) in enumerate(tqdm(data_loader, ascii=True)):\n",
    "        x_train = x[:, :config.n_shot].reshape(-1, *x.shape[-3:]).to(config.device)\n",
    "        y_train = torch.LongTensor(np.repeat(range(config.n_class), config.n_shot)).to(config.device)\n",
    "        x_test = x[:, config.n_shot:].reshape(-1, *x.shape[-3:]).to(config.device)\n",
    "        y_test = torch.LongTensor(np.repeat(range(config.n_class), config.n_eval)).to(config.device)\n",
    "\n",
    "        model_w_grad.reset_batch_stats()\n",
    "        model_wo_grad.reset_batch_stats()\n",
    "        model_w_grad.train()\n",
    "        model_wo_grad.eval()\n",
    "        cI = train_model(model_w_grad, metalearner, x_train, y_train, config)\n",
    "\n",
    "        model_wo_grad.transfer_params(model_w_grad, cI)\n",
    "        output = model_wo_grad(x_test)\n",
    "        loss = model_wo_grad.criterion(output, y_test)\n",
    "        acc = accuracy(output, y_test)\n",
    "\n",
    "        metrics.update(loss=loss.item(), acc=acc, phase='eval')\n",
    "\n",
    "    return metrics.update(eps=eps, total_eps=config.episode_val, phase='eval_done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    def __init__(self, image_size, eps, momentum, n_classes):\n",
    "        self.model = nn.ModuleDict({'features': nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(64, momentum=momentum)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2)),\n",
    "            ('conv2', nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(64, momentum=momentum)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(2, 2)),\n",
    "            ('conv3', nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(64, momentum=momentum)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2)),\n",
    "            ('conv4', nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)),\n",
    "            ('bn4', nn.BatchNorm2d(64, momentum=momentum)),\n",
    "            ('relu4', nn.ReLU(inplace=True)),\n",
    "            ('pool4', nn.MaxPool2d(2, 2))\n",
    "        ]))})\n",
    "        clr_in = image_size // 2**4\n",
    "        self.model.update({'cls': nn.Linear(32 * clr_in * clr_in, n_classes)})\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, ):\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.metalstm = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            loss, grad_p, grad = inputs\n",
    "            loss = loss.expand(grad_p)\n",
    "            inputs = torch.cat((loss, grad_p),1)\n",
    "\n",
    "            lstm_h, lstm_c = self.lstm(inputs)\n",
    "            flat_learner, metalstm_h = self.metalstm([lstm_h, grad],0)\n",
    "            return flat_learner, [(lstm_h, lstm_c), metalstm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_train(learner, metalearner, input, target, dat):\n",
    "    ci = metalearner.metalstm.ci.data\n",
    "\n",
    "    for _ in range(dat['epoch']):\n",
    "        for i in range(0, len(input), dat['batch_size']):\n",
    "            start_idx = i\n",
    "            end_idx = min(len(input), i + dat['batch_size'])\n",
    "            x = input[start_idx:end_idx]\n",
    "            y = target[start_idx:end_idx]\n",
    "\n",
    "            output = learner(x)\n",
    "            loss = learner.crtierion(output,y)\n",
    "            acc = accuracy(output, y)\n",
    "            loss.backward()\n",
    "            grad = torch.cat([p.grad.data.view(-1) / dat['batch_size'] for p in learner.parameters()], 0)\n",
    "\n",
    "            grad_prep = preprocess_grad(grad)\n",
    "            loss_prep = preprocess_loss(loss)\n",
    "            ci, h = metalearner(metalearner_input)\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "dat = []\n",
    "dat['n_shot'] = 5\n",
    "dat['n_eval'] = 15\n",
    "dat['n_class'] = 10\n",
    "dat['input_size'] = 4\n",
    "dat['hidden_size'] = 20\n",
    "dat['lr'] = 1e-3\n",
    "dat['episode'] = 10000\n",
    "dat['epoch'] = 8 \n",
    "dat['batch_size'] = 25\n",
    "dat['image_size'] = 32\n",
    "dat['grad_clip'] = 0.25\n",
    "dat['momentum'] = 0.95\n",
    "dat['eps'] = 1e-3\n",
    "\n",
    "# Load data\n",
    "dat['data_root'] = '/kaggle/input/cifar10_dat/cifar10/test/'\n",
    "\n",
    "loss_list = [] \n",
    "logs = create_logger(dat)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(dat['seed'])\n",
    "np.random.seed(dat['seed'])\n",
    "torch.manual_seed(dat['seed'])\n",
    "\n",
    "# learner & meta learner\n",
    "learner = Learner(dat['input_size'], dat['hidden_size'], dat['n_class']).to(device)\n",
    "meta_learner = MetaLearner(learner, dat['lr'], dat['grad_clip'], dat['momentum'], dat['eps']).to(device)\n",
    "\n",
    "optim = torch.optim.Adam(meta_learner.parameters(),dat['lr'])\n",
    "\n",
    "best_accuracy = 0.0\n",
    "logs.loginfo(\"Begin Training\")\n",
    "\n",
    "# print gpu status and usage\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "# Meta Training\n",
    "\n",
    "for episode, (epx, epy) in enumerate(learn_train):\n",
    "\n",
    "    train_target = torch.LongTensor(np.repeat(range(dat['n_class']), dat['n_shot'])).to(device)\n",
    "    train_input = epx.reshape(-1,*epx.shape).to(device)\n",
    "    test_target = torch.LongTensor(np.repeat(range(dat['n_class']), dat['n_eval'])).to(device)\n",
    "    test_input = epy.reshape(-1,*epy.shape).to(device)\n",
    "    \n",
    "    # Training with MetaLearner\n",
    "    learner_grad.reset_batch_stats()\n",
    "    learner_no_grad.reset_batch_stats()\n",
    "    learner_grad.train()\n",
    "    learner_no_grad.train()\n",
    "    CI = learn_train(learner_grad,metalearner,train_input, train_target, dat)\n",
    "\n",
    "    # Validation loss training\n",
    "    learner_no_grad.transfer_params(learner_grad,CI)\n",
    "    output = learner_no_grad(test_input)\n",
    "    loss = learner_no_grad.criterion(output,test_target)\n",
    "    acc_level = accuracy(output,test_target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    logs.batch_info(eps, dat['episode'],loss.item(), acc_level)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Meta Validation\n",
    "    if eps % dat['val_freq'] ==0 and eps!=0:\n",
    "        save(eps, metalearner,optim, dat['save'])\n",
    "        learner_no_grad.transfer_params(learner_grad,CI)\n",
    "        acc_level = meta_test(eps,val_loader,learner_grad,learner_no_grad,metalearner,dat)\n",
    "\n",
    "\n",
    "torch.save(learner_grad.state_dict(),'learner_grad.pt')\n",
    "torch.save(metalearner.state_dict(),'metalearner.pt')\n",
    "np.savetxt('training_loss.csv', loss_list, delimiter=',')\n",
    "torch.save(optim.state_dict(), 'optimizer.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
